{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9axaYbj_cJ8X"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 /content/kaggle.json"
      ],
      "metadata": {
        "id": "Ar5ian9EcU5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d dmitrybabko/speech-emotion-recognition-en\n",
        "!unzip -q speech-emotion-recognition-en.zip -d dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGQPhRGhcZI0",
        "outputId": "bc9d0af7-c82e-498f-f1db-2a8db8693390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/dmitrybabko/speech-emotion-recognition-en\n",
            "License(s): copyright-authors\n",
            "Downloading speech-emotion-recognition-en.zip to /content\n",
            " 98% 964M/987M [00:11<00:00, 222MB/s]\n",
            "100% 987M/987M [00:11<00:00, 89.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "lWzr-_mHhkms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"/content/dataset/Crema\"\n",
        "EMOTIONS = {\n",
        "    \"ANG\": \"angry\",\n",
        "    \"DIS\": \"disgust\",\n",
        "    \"FEA\": \"fear\",\n",
        "    \"HAP\": \"happy\",\n",
        "    \"NEU\": \"neutral\",\n",
        "    \"SAD\": \"sad\"\n",
        "}\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEIHMi11hpY4",
        "outputId": "ba4c4f92-69db-4924-c568-7efdb3eb4538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_noise(waveform, noise_level=0.005):\n",
        "    noise = noise_level * torch.randn(waveform.size())\n",
        "    return waveform + noise\n",
        "\n",
        "def change_pitch(waveform, sample_rate, semitones=2):\n",
        "    rate = int(sample_rate * (2.0 ** (semitones / 12.0)))\n",
        "    return torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=rate)(waveform)\n",
        "\n",
        "def augment(waveform, sample_rate):\n",
        "    if random.random() < 0.3:\n",
        "        waveform = add_noise(waveform)\n",
        "    if random.random() < 0.3:\n",
        "        waveform = change_pitch(waveform, sample_rate)\n",
        "    return waveform\n"
      ],
      "metadata": {
        "id": "gA5qbdheh0LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_wav2vec_features(file_path, augment_data=False):\n",
        "    waveform, sr = torchaudio.load(file_path)\n",
        "    waveform = waveform.squeeze(0)\n",
        "\n",
        "    if augment_data:\n",
        "        waveform = augment(waveform, sr)\n",
        "\n",
        "    inputs = processor(waveform, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        embeddings = wav2vec(**inputs.to(device)).last_hidden_state.mean(dim=1)  # (1, 768)\n",
        "\n",
        "    return embeddings.cpu().numpy()"
      ],
      "metadata": {
        "id": "CqTpeYlFh4Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_classifier(input_dim=768, num_classes=6):\n",
        "    model = Sequential([\n",
        "        Dense(256, activation='relu', input_shape=(input_dim,)),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "SK9okxYPh5Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "os.makedirs(\"features\", exist_ok=True)\n",
        "os.makedirs(\"labels\", exist_ok=True)\n",
        "\n",
        "X, y = [], []\n",
        "for file in tqdm(os.listdir(DATA_PATH)):\n",
        "    if not file.endswith(\".wav\"):\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        emotion_key = file.split(\"_\")[2]\n",
        "        emotion = EMOTIONS.get(emotion_key)\n",
        "        if not emotion:\n",
        "            continue\n",
        "\n",
        "        file_path = os.path.join(DATA_PATH, file)\n",
        "        feat_path = f\"features/{file.replace('.wav', '.npy')}\"\n",
        "        label_path = f\"labels/{file.replace('.wav', '.label')}\"\n",
        "\n",
        "        if os.path.exists(feat_path) and os.path.exists(label_path):\n",
        "            vec = np.load(feat_path)\n",
        "            with open(label_path) as f:\n",
        "                emotion = f.read().strip()\n",
        "        else:\n",
        "            vec = extract_wav2vec_features(file_path, augment_data=False)\n",
        "            np.save(feat_path, vec)\n",
        "            with open(label_path, \"w\") as f:\n",
        "                f.write(emotion)\n",
        "\n",
        "        X.append(vec.squeeze())\n",
        "        y.append(emotion)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping {file}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkGhRTRwtYE1",
        "outputId": "adc28f2b-eff2-4fa7-c8dd-7305cef894fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7442/7442 [03:21<00:00, 36.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "y_cat = to_categorical(y_encoded)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(np.array(X), y_cat, test_size=0.2, stratify=y_cat)\n",
        "\n",
        "model = get_classifier(input_dim=X_train.shape[1], num_classes=y_cat.shape[1])\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "re2GHLwzi0Dd",
        "outputId": "65f1508c-7a6f-4ab3-97f5-c8428d4e55a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.3312 - loss: 1.5747 - val_accuracy: 0.4493 - val_loss: 1.3884\n",
            "Epoch 2/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.4441 - loss: 1.3854 - val_accuracy: 0.4728 - val_loss: 1.3330\n",
            "Epoch 3/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4579 - loss: 1.3430 - val_accuracy: 0.4856 - val_loss: 1.3081\n",
            "Epoch 4/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4720 - loss: 1.3199 - val_accuracy: 0.5131 - val_loss: 1.2665\n",
            "Epoch 5/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4882 - loss: 1.2665 - val_accuracy: 0.5003 - val_loss: 1.2690\n",
            "Epoch 6/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5113 - loss: 1.2406 - val_accuracy: 0.5212 - val_loss: 1.2320\n",
            "Epoch 7/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5315 - loss: 1.2059 - val_accuracy: 0.5265 - val_loss: 1.2124\n",
            "Epoch 8/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5268 - loss: 1.2068 - val_accuracy: 0.5400 - val_loss: 1.1956\n",
            "Epoch 9/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.5469 - loss: 1.1709 - val_accuracy: 0.5668 - val_loss: 1.1483\n",
            "Epoch 10/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5554 - loss: 1.1417 - val_accuracy: 0.5453 - val_loss: 1.1749\n",
            "Epoch 11/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5591 - loss: 1.1313 - val_accuracy: 0.5507 - val_loss: 1.1724\n",
            "Epoch 12/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5530 - loss: 1.1489 - val_accuracy: 0.5567 - val_loss: 1.1429\n",
            "Epoch 13/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5639 - loss: 1.1326 - val_accuracy: 0.5473 - val_loss: 1.1584\n",
            "Epoch 14/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5799 - loss: 1.0870 - val_accuracy: 0.5668 - val_loss: 1.1563\n",
            "Epoch 15/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5681 - loss: 1.1039 - val_accuracy: 0.5769 - val_loss: 1.1102\n",
            "Epoch 16/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5647 - loss: 1.1184 - val_accuracy: 0.5964 - val_loss: 1.0956\n",
            "Epoch 17/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5845 - loss: 1.0714 - val_accuracy: 0.5702 - val_loss: 1.1186\n",
            "Epoch 18/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5791 - loss: 1.0846 - val_accuracy: 0.5944 - val_loss: 1.0779\n",
            "Epoch 19/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5772 - loss: 1.0899 - val_accuracy: 0.5923 - val_loss: 1.1006\n",
            "Epoch 20/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.5972 - loss: 1.0624 - val_accuracy: 0.5621 - val_loss: 1.1354\n",
            "Epoch 21/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5887 - loss: 1.0637 - val_accuracy: 0.5910 - val_loss: 1.0905\n",
            "Epoch 22/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5879 - loss: 1.0681 - val_accuracy: 0.5702 - val_loss: 1.1317\n",
            "Epoch 23/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5832 - loss: 1.0766 - val_accuracy: 0.5668 - val_loss: 1.1278\n",
            "Epoch 24/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5927 - loss: 1.0576 - val_accuracy: 0.5695 - val_loss: 1.1289\n",
            "Epoch 25/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5876 - loss: 1.0598 - val_accuracy: 0.5796 - val_loss: 1.1256\n",
            "Epoch 26/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6085 - loss: 1.0390 - val_accuracy: 0.5823 - val_loss: 1.1159\n",
            "Epoch 27/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6011 - loss: 1.0366 - val_accuracy: 0.6024 - val_loss: 1.0575\n",
            "Epoch 28/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6272 - loss: 0.9906 - val_accuracy: 0.6165 - val_loss: 1.0531\n",
            "Epoch 29/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.6321 - loss: 0.9835 - val_accuracy: 0.5641 - val_loss: 1.1146\n",
            "Epoch 30/30\n",
            "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6136 - loss: 0.9965 - val_accuracy: 0.5749 - val_loss: 1.1002\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7deac4286250>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"wav2vec_classifier3 .keras\")\n",
        "np.save(\"label_classes3.npy\", le.classes_)"
      ],
      "metadata": {
        "id": "IdBsjUsPjAa-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}